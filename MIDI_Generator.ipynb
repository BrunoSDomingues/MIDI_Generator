{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3459fe2",
   "metadata": {},
   "source": [
    "# Gerador de Músicas MIDI\n",
    "\n",
    "O projeto consiste em realizar um estudo no qual serão geradas músicas sem intervenção humana. Para tal, utilizou-se de músicas clássicas de Chopin como dataset e um modelo de Deep Learning.\n",
    "\n",
    "## Deep Learning - Uma breve introdução\n",
    "\n",
    "O Deep Learning é um algoritmo de Machine Learning, no qual são utilizadas redes neurais que buscam \"aprender\" de forma similar ao comportamento humano.\n",
    "\n",
    "Uma forma simples de entender é imaginar uma criança aprendendo a reconhecer objetos. A criança pode apontar para um objeto e dizer que é um carro. Dado isto, o pai/a mãe da criança pode reagir de duas maneiras: confirmar que o objeto que a criança apontou é um carro, ou falar \"Não, isto é um jarro\". Ao receber feedback suficiente, a criança começa a internalizar as características de cada objeto e cria um modelo mental que ajuda ela a reconhecer os diferentes objetos. Este modelo depende de uma comunicação efetiva entre os neurônios, transmitindo diferentes sinais e gerando este modelo complexo e hierárquico baseado no feedback recebido.\n",
    "\n",
    "O Deep Learning busca replicar este comportamento, criando um modelo no qual não é necessário entender cada etapa e decisão feita devido à complexidade e profundidade deste. Para criar o modelo, são utilizadas múltiplas camadas de \"neurônios digitais\", que vão repassando o aprendizado de camada em camada.\n",
    "\n",
    "Inicialmente, o modelo é alimentado com os dados a serem utilizados, e este tenta prever os dados, sem nenhuma intervenção. As previsões iniciais irão ser completamente (ou em sua maior parte) incorretas, mas conforme o modelo recebe feedback de suas previões, ele ajusta a comunicação entre seus \"neurônios\" até ser capaz de gerar previsões mais acuradas.\n",
    "\n",
    "Existem diversos modelos de Deep Learning, e para o desenvolvimentod este projeto, irá ser utilizado o modelo de WaveNet.\n",
    "\n",
    "## WaveNet\n",
    "\n",
    "O WaveNet é um modelo de Deep Learning para áudios \"crus\" desenvolvido pelo Google DeepMind. Ele é chamado de \"generative model\", pois tem como objetivo gerar novos samples a partir da distribuição original dos dados. Ele atua de forma similar aos modelos de linguagem utilizados em NLP.\n",
    "\n",
    "### Treinando o WaveNet\n",
    "\n",
    "Para treinar o modelo de WaveNet, utiliza-se um trecho de uma onda crua de áudio (no caso, a onda de áudio no domínio do tempo) como input. Uma onda de áudio no domínio do tempo é representada na forma de diferente valores de amplitude em diferentes intervalos de tempo, como é possível visualizar no gif abaixo.\n",
    "\n",
    "![Onda de áudio no domínio do tempo](https://jvbalen.github.io/figs/wavenet.gif)\n",
    "\n",
    "A partir da sequência de valores de amplitude, o WaveNet tenta prever qual valor de amplitude vem em seguida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "# Numpy for arrays and matplotlib for notes histogram\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Music21 library for MIDI reading and creating\n",
    "import music21 as m21\n",
    "from music21.note import Note\n",
    "from music21.chord import Chord\n",
    "\n",
    "# sklearn to split train and test \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_folder_path = \"chopin/\"\n",
    "threaded = True # Esquenta o computador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624bebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    # keras to train the model\n",
    "    import keras.backend as kb\n",
    "    import keras.callbacks as kc\n",
    "    import keras.layers as kl\n",
    "    import keras.models as km\n",
    "else:\n",
    "    # keras to load the model only\n",
    "    from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c19ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):   \n",
    "    notes = []\n",
    "    \n",
    "    # Parsing the MIDI file\n",
    "    midi = m21.converter.parse(file)\n",
    "  \n",
    "    # Partition by instrument\n",
    "    partition = m21.instrument.partitionByInstrument(midi)\n",
    "\n",
    "    # Looping over all the instruments\n",
    "    for part in partition.parts:\n",
    "        # Select only the piano\n",
    "        if 'Piano' not in str(part):\n",
    "            continue\n",
    "\n",
    "        # Checking if it is a note or a chord\n",
    "        for element in part.recurse():\n",
    "            if isinstance(element, Note):\n",
    "                notes.append(str(element.pitch))\n",
    "\n",
    "            elif isinstance(element, Chord):\n",
    "                notes.append('.'.join(map(str, element.normalOrder)))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01016",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = (p for p in Path(extracted_folder_path).rglob(\"*\") if p.is_file() and p.suffix == \".mid\")\n",
    "\n",
    "if threaded:\n",
    "    from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "    with ProcessPoolExecutor() as pool:\n",
    "        notes_list = list(pool.map(read_midi, files))\n",
    "else:\n",
    "    notes_list = list(map(read_midi, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening notes_list\n",
    "notes_f = np.concatenate(notes_list).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b999f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of each note\n",
    "unique_notes, counts = np.unique(notes_f, return_counts=True)\n",
    "\n",
    "print(f\"Number of unique notes: {len(unique_notes)}\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ce82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most frequent notes\n",
    "frequent_notes = frozenset(unique_notes[counts >= 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert note to int and vice-versa\n",
    "\n",
    "# Use tee to guarantee the iterators are the same\n",
    "enum_it, rev_enum_it = tee(enumerate(frequent_notes))\n",
    "\n",
    "# Helper functions\n",
    "d_int_to_note = dict(enum_it)\n",
    "int_to_note = lambda idx: d_int_to_note[idx]\n",
    "\n",
    "d_note_to_int = {note: idx for idx, note in rev_enum_it}\n",
    "note_to_int = lambda idx: d_note_to_int[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music = []\n",
    "\n",
    "# Adding the most frequent notes\n",
    "for notes in notes_list:\n",
    "    new_music.append([d_note_to_int[note] for note in notes if note in frequent_notes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 32\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for note in new_music:\n",
    "    for start in range(len(note) - n_timesteps):\n",
    "        end = start + n_timesteps\n",
    "        X.append(note[start:end])  # Input\n",
    "        y.append(note[end])        # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(X), np.array(y), test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    kb.clear_session()\n",
    "    model = km.Sequential()\n",
    "\n",
    "    # TODO: Validate x.unique() = frequent_notes\n",
    "    model.add(kl.Embedding(len(frequent_notes), 100, input_length=32, trainable=True)) \n",
    "\n",
    "    model.add(kl.Conv1D(64, 3, padding='causal',activation='relu'))\n",
    "    model.add(kl.Dropout(0.2))\n",
    "    model.add(kl.MaxPool1D(2))\n",
    "\n",
    "    model.add(kl.Conv1D(128, 3, activation='relu', dilation_rate=2, padding='causal'))\n",
    "    model.add(kl.Dropout(0.2))\n",
    "    model.add(kl.MaxPool1D(2))\n",
    "\n",
    "    model.add(kl.Conv1D(256, 3, activation='relu', dilation_rate=4, padding='causal'))\n",
    "    model.add(kl.Dropout(0.2))\n",
    "    model.add(kl.MaxPool1D(2))\n",
    "\n",
    "    model.add(kl.GlobalMaxPool1D())\n",
    "\n",
    "    model.add(kl.Dense(256, activation='relu'))\n",
    "    # TODO: Validate y.unique() = frequent_notes\n",
    "    model.add(kl.Dense(len(frequent_notes), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    mc = kc.ModelCheckpoint('model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "    history = model.fit(X_train, y_train, batch_size=128, epochs=50, validation_data=(X_val, y_val), verbose=1, callbacks=[mc])\n",
    "else:\n",
    "    model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d25d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, X_val.shape[0] - 1)\n",
    "\n",
    "random_music = X_val[idx]\n",
    "predictions = []\n",
    "\n",
    "for i in range(10):\n",
    "    random_music = random_music.reshape(1, n_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred = np.argmax(prob, axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda13245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting previously generated MIDI file, if any\n",
    "midi_file = \"music.mid\"\n",
    "Path(midi_file).unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a280bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output, mf):\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for offset, pattern in enumerate(prediction_output):\n",
    "        # pattern is a chord\n",
    "        if '.' in pattern or pattern.isdigit():\n",
    "            notes = []\n",
    "            \n",
    "            for current_note in pattern.split('.'):\n",
    "                new_note = Note(int(current_note))\n",
    "                new_note.storedInstrument = m21.instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = m21.instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "    midi_stream = m21.stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MIDI file\n",
    "convert_to_midi(map(int_to_note, predictions), midi_file)\n",
    "\n",
    "# Shows MIDI in player\n",
    "mf = m21.midi.MidiFile()\n",
    "mf.open(midi_file)\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = m21.midi.translate.midiFileToStream(mf)\n",
    "s.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0b919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
